
# Kafka Topics

Topics: a particular stream of data (similar to a what a table would be in a database without all the constraints)
You can have as many topics as you want
It is defined by its name
These topics support any kind of message format, JSON, AVRO, TEXT, Binary...
The sequence of messages is called a data stream
You can not query topics, instead, use Kafka Producers to send data and Kafka Consumers to read the data

Topics are split in partitions (example 100 partitions)

Messages within each partition are ordered
Each message within a partition gets an incremental id, called offset

### Kafka topics are <b>immutable</b>: once data is written to a partition, it cannot be changed (data can't be deleted / updated)

Once the data is written to a partition, it cannot be changed (immutability)
Data is kept only for a limited time (default is one week - configurable)
Offsets only have a meaning for a specific partition
Offsets are not re-used even if previous messages have been deleted
Order is guaranteed only with a partition (not across partitions)
Data is assigned randomly to a partition unless a key is provided
We can have as many partitions per topic as we want

---

# Producers and Message keys

Producers write data to topics (which are made of partitions)
Producers know to which partition to write to (and which Kafka broker has it)

Producers can choose to send a KEY with message (string, number, binary, etc...)

if key = null, data is sent round robin (partition 0, then 1, then 2...)

if key != null then it has some value, so all the messages for that key will always go to the same partition (hashing)

Key is typically sent if we want message ordering for a specific field (ex: truck_id)

---

### Kafka Message Anatomy

Key, Value, Compression Type, Headers (Optional), Partition + Offset, Timestamp (System or user set) -> sent to Kafka


### Kafka Message Serializer

Kafka only accepts bytes as an input from producers and sends bytes as an output to consumers

Serialization: Transform objects / data into bytes

They are used on the value and the key only

Common Serializers:
	String
	Int,Float
	Avro
	Protobuf

# Consumers & Deserialization

Consumers read data from a topic (identified by name)  - Pull Model
Consumers automatically know which broker to read from
In case of broker failures, consumers know how to recover
Data is read in order from low to high offset within each partitions

## Consumer Deserializer

Deserialize indicates how to transform bytes into objects / data

They are used on the value and the key of the message

Same deserialisers as serializers
